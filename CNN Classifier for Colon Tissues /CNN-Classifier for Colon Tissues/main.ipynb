{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "HW3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "sGqabSidl1mI"
      },
      "source": [
        "def train_model(image_datasets, path_keys, model, criterion, optimizer, scheduler, num_epochs):\n",
        "\n",
        "  print(\"---TRAINING---\")\n",
        "  dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size = 4, shuffle = True, num_workers = 4)\n",
        "                for x in path_keys}\n",
        "\n",
        "  best_model_wts = copy.deepcopy(model.state_dict()) \n",
        "  best_no_corrects= 0\n",
        "\n",
        "  classes = [0,1,2]\n",
        "  class_correct = list(0. for i in range(len(classes)))\n",
        "  class_total = list(0. for i in range(len(classes)))\n",
        "  c = 0\n",
        "\n",
        "  # for validation\n",
        "  classes2 = [0,1,2]\n",
        "  class_correct2 = list(0. for i in range(len(classes)))\n",
        "  class_total2 = list(0. for i in range(len(classes)))\n",
        "  c2 = 0\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "    # Set the model to the training mode for updating the weights using # the first portion of training images\n",
        "    running_loss = 0\n",
        "    model.train()\n",
        "    for inputs, labels in dataloaders[path_keys[0]]: # iterate over data\n",
        "      inputs = inputs.to(device)\n",
        "      labels = labels.to(device) \n",
        "      optimizer.zero_grad()\n",
        "      with torch.set_grad_enabled(True):\n",
        "        outputs = model(inputs)\n",
        "        _, predicted = torch.max(outputs, 1) \n",
        "        c = (predicted == labels).squeeze()\n",
        "        loss = criterion(outputs, labels) \n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "  \n",
        "        for i in range(len(labels)):\n",
        "          label = labels[i]\n",
        "          if len(c.size()) != 0:\n",
        "            class_correct[label] += c[i].item()\n",
        "          else:\n",
        "            class_correct[label] += c\n",
        "          class_total[label] += 1\n",
        "\n",
        "  # Set the model to the evaluation mode for selecting the best network # based on the number of correctly classified validation images \n",
        "   \n",
        "    model.eval()\n",
        "    no_corrects = 0\n",
        "    for inputs2, labels2 in dataloaders[path_keys[1]]: \n",
        "      inputs2 = inputs2.to(device)\n",
        "      labels2 = labels2.to(device)\n",
        "      with torch.set_grad_enabled(False):\n",
        "        outputs2 = model(inputs2)\n",
        "        _, predicted2 = torch.max(outputs2, 1)\n",
        "        c2 = (predicted2 == labels2).squeeze()\n",
        "        no_corrects += torch.sum(predicted2 == labels2.data)\n",
        "\n",
        "        for i in range(len(labels2)):\n",
        "          label2 = labels2[i]\n",
        "          if len(c2.size()) != 0:\n",
        "            class_correct2[label2] += c2[i].item()\n",
        "          else:\n",
        "            class_correct2[label] += c2\n",
        "          class_total2[label2] += 1\n",
        "    \n",
        "    if no_corrects > best_no_corrects:\n",
        "      best_no_corrects = no_corrects\n",
        "      best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    \n",
        "    scheduler.step()\n",
        "    # Load the weights of the best network\n",
        "  \n",
        "  # train accuracy\n",
        "  print()\n",
        "  print(\"TRAIN ACCURACY\")\n",
        "  print()\n",
        "  for i in range(3):\n",
        "    if class_total[i] != 0 :\n",
        "      print('Accuracy of %5s : %2d %%' % (\n",
        "          classes[i], 100 * class_correct[i] / class_total[i]))\n",
        "    else: \n",
        "      print(\"Class total is 0!\")\n",
        "  print(\"Overall accuracy is: \" , (class_correct[0] +  class_correct[1] + class_correct[2]) / (class_total[0] + class_total[1] +class_total[2]))    \n",
        "  \n",
        "  # print(\"class_correct[0]\", class_correct[0])\n",
        "  # print(\"class_correct[1]\", class_correct[1])\n",
        "  # print(\"class_correct[2]\", class_correct[2])\n",
        "\n",
        "  # print(\"-------------------------\")\n",
        "    \n",
        "  # print(\"class_total[0]\", class_total[0])\n",
        "  # print(\"class_total[1]\", class_total[1])\n",
        "  # print(\"class_total[2]\", class_total[2])\n",
        "\n",
        "\n",
        "  print(\"-------------------------\")\n",
        "  print(\"-------------------------\")\n",
        " \n",
        "  # validation accuracy\n",
        "  print(\"VALIDATION ACCURACY\")\n",
        "  print()\n",
        "  for i in range(3):\n",
        "    if class_total2[i] != 0 :\n",
        "      print('Accuracy of %5s : %2d %%' % (\n",
        "          classes2[i], 100 * class_correct2[i] / class_total2[i]))\n",
        "    else: \n",
        "      print(\"Class total is 0!\")\n",
        "\n",
        "  print(\"Overall accuracy is: \" , (class_correct2[0] +  class_correct2[1] + class_correct2[2]) / (class_total2[0] + class_total2[1] +class_total2[2]))    \n",
        "  \n",
        "  # print(\"class_correct[0]\", class_correct2[0])\n",
        "  # print(\"class_correct[1]\", class_correct2[1])\n",
        "  # print(\"class_correct[2]\", class_correct2[2])\n",
        "\n",
        "  # print(\"-------------------------\")\n",
        "    \n",
        "  # print(\"class_total[0]\", class_total2[0])\n",
        "  # print(\"class_total[1]\", class_total2[1])\n",
        "  # print(\"class_total[2]\", class_total2[2])\n",
        "\n",
        "  print()\n",
        "  print()\n",
        "\n",
        "  # calc_accuracy(class_total, class_correct) \n",
        "  model.load_state_dict(best_model_wts) \n",
        "  return model\n"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6tYFs2Q1xLpN"
      },
      "source": [
        "def test(image_datasets, model, path_keys):\n",
        " \n",
        "  c = 0\n",
        "  inputs = 0\n",
        "  outputs = 0\n",
        "  predicted = 0\n",
        "  class_total = 0\n",
        "  class_correct = 0\n",
        "\n",
        "  #Testing classification accuracy for individual classes.\n",
        "\n",
        "  dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size = 4, shuffle = True, num_workers = 4)\n",
        "                for x in path_keys}\n",
        "\n",
        "  classes = [0,1,2]\n",
        "  class_correct = list(0. for i in range(len(classes)))\n",
        "  class_total = list(0. for i in range(len(classes)))\n",
        "\n",
        "  model.eval()\n",
        "  print(\"Eval active\")\n",
        "  with torch.no_grad():\n",
        "    for inputs, labels in dataloaders[path_keys[2]]:\n",
        "    \n",
        "      inputs = inputs.to(device)\n",
        "      labels = labels.to(device) \n",
        "      outputs = model(inputs)\n",
        "      _, predicted = torch.max(outputs, 1)\n",
        "      c = (predicted == labels).squeeze()\n",
        "      # class_total, class_correct = check_labels(labels, c)\n",
        "  \n",
        "      for i in range(len(labels)):\n",
        "        label = labels[i]\n",
        "        if len(c.size()) != 0:\n",
        "          class_correct[label] += c[i].item()\n",
        "        else:\n",
        "          class_correct[label] += c\n",
        "        class_total[label] += 1\n",
        "\n",
        "  print(\"---TEST ACCURACY---\")\n",
        "  for i in range(3):\n",
        "    if class_total[i] != 0 :\n",
        "      print('Accuracy of %5s : %2d %%' % (\n",
        "          classes[i], 100 * class_correct[i] / class_total[i]))\n",
        "    else: \n",
        "      print(\"Class total is 0!\")\n",
        "  print(\"Overall accuracy is: \" , (class_correct[0] +  class_correct[1] + class_correct[2]) / (class_total[0] + class_total[1] +class_total[2]))    \n",
        "  \n",
        "\n",
        "  # calc_accuracy(class_total, class_correct)  \n",
        "\n",
        "  # print(\"class_correct[0]\", class_correct[0])\n",
        "  # print(\"class_correct[1]\", class_correct[1])\n",
        "  # print(\"class_correct[2]\", class_correct[2])\n",
        "\n",
        "  # print(\"-------------------------\")\n",
        "    \n",
        "  # print(\"class_total[0]\", class_total[0])\n",
        "  # print(\"class_total[1]\", class_total[1])\n",
        "  # print(\"class_total[2]\", class_total[2])\n"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VhFxf0chl6T9"
      },
      "source": [
        "def data_transforms(is_normalized, mean_arr, std_arr, path_keys):\n",
        "  if is_normalized:\n",
        "    return  {     path_keys[0]: transforms.Compose([\n",
        "                  transforms.ToTensor(),\n",
        "                  transforms.Normalize(mean = mean_arr[0], std = std_arr[0])\n",
        "                  ]),\n",
        "              \n",
        "                  path_keys[1]: transforms.Compose([\n",
        "                  transforms.ToTensor(),      \n",
        "                  transforms.Normalize(mean = mean_arr[1], std = std_arr[1])\n",
        "                  ]),\n",
        "                  path_keys[2]: transforms.Compose([\n",
        "                  transforms.ToTensor(),                                       \n",
        "                  transforms.Normalize(mean = mean_arr[2], std = std_arr[2])\n",
        "                  ])\n",
        "                  }\n",
        "  else: \n",
        "    return {      path_keys[0]: transforms.Compose([\n",
        "                  transforms.ToTensor(),\n",
        "           \n",
        "                 \n",
        "                  ]),\n",
        "              \n",
        "                  path_keys[1]: transforms.Compose([\n",
        "                  transforms.ToTensor(),\n",
        "                           \n",
        "           \n",
        "                  ]),\n",
        "                  path_keys[2]: transforms.Compose([\n",
        "                  transforms.ToTensor(),                                     \n",
        "        \n",
        "                  ])\n",
        "                  }\n"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ll5XqKjbq-To",
        "outputId": "1e138ed0-c9f7-4088-f199-8a617a0d4d55"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cw9ahy5Qtjtt"
      },
      "source": [
        "import torch \n",
        "import numpy\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "from skimage import io\n",
        "import os\n",
        "from __future__ import print_function, division\n",
        "from torch.optim import lr_scheduler\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "import time\n",
        "import copy\n",
        "from torch import Tensor, autograd\n",
        "from skimage import color\n",
        "import cv2\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torchsummary import summary\n",
        "import pdb"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7wjGF_F1mVGb"
      },
      "source": [
        "class_no = 3\n",
        "\n",
        "num_epochs = 4\n",
        "\n",
        "LEARNING_RATE = 0.1\n",
        "path_dataset = \"/content/drive/MyDrive/COMP448-HW3/dataset_valid_vers\"\n",
        "\n",
        "\n",
        "sub_path_balanced_train_img = \"/balanced_train/\" \n",
        "sub_path_balanced_valid_img = \"/balanced_valid/\"\n",
        "sub_path_imbalanced_train_img = \"/imbalanced_train/\" \n",
        "sub_path_imbalanced_valid_img = \"/imbalanced_valid/\"\n",
        "sub_path_test_img =  \"/test/\"\n",
        "\n",
        "folder_path_balanced_train_img = path_dataset + sub_path_balanced_train_img \n",
        "folder_path_balanced_valid_img = path_dataset + sub_path_balanced_valid_img\n",
        "folder_path_imbalanced_train_img = path_dataset + sub_path_imbalanced_train_img \n",
        "folder_path_imbalanced_valid_img = path_dataset + sub_path_imbalanced_valid_img\n",
        "folder_path_test_img = path_dataset + sub_path_test_img\n"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WL9N7eOeUwTE"
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model_conv = models.alexnet(pretrained = True)\n",
        "\n",
        "for param in model_conv.parameters():\n",
        "  param.requires_grad = False\n",
        "\n",
        "model_conv.classifier[6] = nn.Linear(4096, 3)\n",
        "model_conv = model_conv.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = torch.optim.SGD(model_conv.parameters(), lr=LEARNING_RATE, momentum=0.9)\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)  "
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yt1iRsb7_xYB"
      },
      "source": [
        "mean_arr=[0.485, 0.456, 0.406]\n",
        "std_arr=[0.229, 0.224, 0.225]\n"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "luAktkQZe_9j"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bBTUP00Ix0hX"
      },
      "source": [
        "\n",
        "balanced_path_keys = [sub_path_balanced_train_img, sub_path_balanced_valid_img, sub_path_test_img]\n",
        "imbalanced_path_keys = [sub_path_imbalanced_train_img, sub_path_imbalanced_valid_img, sub_path_test_img]\n",
        "\n",
        "normalized_balanced_image_datasets = {x: datasets.ImageFolder(path_dataset + x, data_transforms(True, mean_arr, std_arr, balanced_path_keys)[x]) for x in balanced_path_keys}\n",
        "normalized_imbalanced_image_datasets = {x: datasets.ImageFolder(path_dataset + x, data_transforms(True, mean_arr, std_arr, imbalanced_path_keys)[x]) for x in imbalanced_path_keys}\n",
        "not_normalized_balanced_image_datasets = {x: datasets.ImageFolder(path_dataset + x, data_transforms(False, mean_arr, std_arr, balanced_path_keys)[x]) for x in balanced_path_keys}\n",
        "not_normalized_imbalanced_image_datasets = {x: datasets.ImageFolder(path_dataset + x, data_transforms(False, mean_arr, std_arr, imbalanced_path_keys)[x]) for x in imbalanced_path_keys}\n",
        "\n"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VE8Mv8vkoLTZ"
      },
      "source": [
        ""
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qFLjW4V7paS1"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dmgP7ygmltoP",
        "outputId": "0270f1a0-6a4c-4122-f8d3-0f4862e2c450"
      },
      "source": [
        "## TRAINING \n",
        "# print(\"Normalized balanced model: \")\n",
        "# normalized_balanced_model = train_model(normalized_balanced_image_datasets, balanced_path_keys, model_conv, criterion, optimizer, exp_lr_scheduler, 4)\n",
        "# print()\n",
        "# normalized_balanced_model = normalized_balanced_model.to(device)\n",
        "\n",
        "# print(\"Normalized imbalanced model: \")\n",
        "# normalized_imbalanced_model = train_model(normalized_imbalanced_image_datasets, imbalanced_path_keys, model_conv, criterion, optimizer, exp_lr_scheduler, 4)\n",
        "# print()\n",
        "# normalized_imbalanced_model = normalized_imbalanced_model.to(device)\n",
        "\n",
        "# print(\"Not normalized balanced model: \")\n",
        "# not_normalized_balanced_model = train_model(not_normalized_balanced_image_datasets, balanced_path_keys, model_conv, criterion, optimizer, exp_lr_scheduler, 4)\n",
        "# print()\n",
        "# not_normalized_balanced_model = not_normalized_balanced_model.to(device)\n",
        "\n",
        "# #  below the case is not included\n",
        "# print(\"Not normalized imbalanced model: \")\n",
        "# not_normalized_imbalanced_model = train_model(not_normalized_imbalanced_image_datasets, imbalanced_path_keys, model_conv, criterion, optimizer, exp_lr_scheduler, 4)\n",
        "# not_normalized_imbalanced_model = normalized_imbalanced_model.to(device)\n"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Not normalized balanced model: \n",
            "---TRAINING---\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "TRAIN ACCURACY\n",
            "\n",
            "Accuracy of     0 : 95 %\n",
            "Accuracy of     1 : 84 %\n",
            "Accuracy of     2 : 96 %\n",
            "Overall accuracy is:  tensor(0.9202, device='cuda:0')\n",
            "-------------------------\n",
            "-------------------------\n",
            "VALIDATION ACCURACY\n",
            "\n",
            "Accuracy of     0 : 100 %\n",
            "Accuracy of     1 : 88 %\n",
            "Accuracy of     2 : 100 %\n",
            "Overall accuracy is:  0.9607843137254902\n",
            "\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tSlSxQJWTRxr",
        "outputId": "3432fcc7-16ec-41b5-a7f2-bf7f25bb3836"
      },
      "source": [
        "# TESTING\n",
        "print()\n",
        "## since test data set is same in every dataset, normalized_balanced_image_datasets is used for all test results.\n",
        "# print(\"Normalized balanced test: \")\n",
        "# test(normalized_balanced_image_datasets,normalized_balanced_model, balanced_path_keys)\n",
        "# print()\n",
        "# print(\"Normalized imbalanced test: \")\n",
        "# test(normalized_balanced_image_datasets,normalized_imbalanced_model, balanced_path_keys)\n",
        "# print()\n",
        "print(\"Not_normalized balanced test: \")\n",
        "test(normalized_balanced_image_datasets,not_normalized_balanced_model, balanced_path_keys)\n",
        "\n",
        "\n",
        "## below the case is not included\n",
        "# test(normalized_balanced_image_datasets,not_normalized_imbalanced_model, balanced_path_keys)"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Not_normalized balanced test: \n",
            "Eval active\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "---TEST ACCURACY---\n",
            "Accuracy of     0 : 89 %\n",
            "Accuracy of     1 : 92 %\n",
            "Accuracy of     2 : 79 %\n",
            "Overall accuracy is:  0.8819444444444444\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7XJmv5GCq4QW"
      },
      "source": [
        "# "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}