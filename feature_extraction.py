# -*- coding: utf-8 -*-
"""Feature_Extraction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Jgxdit_eCutscA_6GkCJ8RpXLFkWVRza
"""

# Commented out IPython magic to ensure Python compatibility.


import pandas as pd
import seaborn as sb
import matplotlib.pyplot as plt
import numpy as np

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier 
from sklearn.metrics import confusion_matrix  
from sklearn.metrics import accuracy_score 
from sklearn.feature_selection import SelectKBest 
from sklearn.feature_selection import chi2 
from sklearn.feature_selection import RFE 
from sklearn.feature_selection import RFECV 
from sklearn.decomposition import PCA 
from sklearn import preprocessing 
# %matplotlib inline

aus = pd.read_csv("/content/drive/MyDrive/A-Kaggle-Datasets/data/weatherAUS.csv")
aus = aus.drop(['Location','Date','Evaporation','Sunshine', 'Cloud9am','Cloud3pm',
                           'WindGustDir','WindGustSpeed','WindDir9am','WindDir3pm','WindSpeed9am',
                           'WindSpeed3pm'],axis=1)

X = aus
X["RainToday"] = X.RainToday.replace({"Yes":1,"No":0})
X["RainTomorrow"] = X.RainTomorrow.replace({"Yes":1,"No":0})

y = X.RainTomorrow
X = X.drop(["RainTomorrow"],axis=1)
X = X.fillna(0)
y = y.fillna(0)

#scaling the data
scaler = preprocessing.MinMaxScaler()
scaler.fit(X)
X_trans = pd.DataFrame(scaler.fit_transform(X), columns = X.columns)

X_train,X_test,y_train,y_test = train_test_split(X_trans,y,test_size=0.25,random_state=43)

X_trans.head()

# 2. Univariate Feature Selection

X_train
SelectKBest(chi2,k=5)

Univariate_feature_select = SelectKBest(chi2,k=5).fit(X_train,y_train)
dict = {key: value for (key,value) in zip (Univariate_feature_select.scores_,X_train.columns)}
sorted(dict.items())

print("Shape of original data: ", X_train.shape)
print("Shape of corpus with best features: ", X_train_k_best.shape)

#extract the best K values
X_train_k_best = Univariate_feature_select.transform(X_train)
X_test_k_best = Univariate_feature_select.transform(X_test)


# Testing with random forest algorithm
RandForest_K_best = RandomForestClassifier()
RandForest_K_best = RandForest_K_best.fit(X_train_k_best, y_train)
y_pred = RandForest_K_best.predict(X_test_k_best)
accuracy = accuracy_score(y_test, y_pred)
accuracy

conf_matrix = confusion_matrix(y_test,y_pred)
sb.heatmap(conf_matrix, annot=True, fmt="d")

# 3. Recursive Feature Elimination
# test with different sizes of features
RandForest_RFE = RandomForestClassifier()
rfe = RFE(estimator=RandForest_RFE, n_features_to_select=5,step=1)
rfe = rfe.fit(X_train, y_train)

print("Best Features Choosen by RFE: \n")
for i in X_train.columns[rfe.support_]:
    print(i)

# obtaining x_train and X_test based on best features
X_train_RFE = rfe.transform(X_train)
X_test_RFE = rfe.transform(X_test)

RandForest_RFE = RandForest_RFE.fit(X_train_RFE, y_train)
y_pred = RandForest_RFE.predict(X_test_RFE)
accuracy_score(y_test,y_pred)
conf_matrix = confusion_matrix(y_test,y_pred)
sb.heatmap(conf_matrix, annot=True, fmt="d")

# 4. Recursive Feature elimination with Cross Validation
RandForest_RFECV = RandomForestClassifier()
# setting the RFECV function using the estimator
rfecv = RFECV(estimator=RandForest_RFECV, step=1, cv=3, scoring="accuracy")
rfecv = rfecv.fit(X_train, y_train)
rfecv.n_features_
for i in X_train.columns[rfecv.support_]:
    print(i)

plt.figure()
plt.xlabel("Number of Selected Feature")
plt.ylabel("Score of Selected Features")
plt.plot(range(1,len(rfecv.grid_scores_)+1), rfecv.grid_scores_)
plt.show()

# 5 Tree base Feature Selection



RandForest_Tree = RandomForestClassifier()

RandForest_Tree = RandForest_Tree.fit(X_train, y_train)
relevants = RandForest_Tree.feature_importances_



# argsort the arrays, get the scores sorted by the saving the place they have
std = np.std([tree.feature_importances_ for tree in RandForest_Tree.estimators_], axis = 0)
indices = np.argsort(relevants)[::-1]


for i in range(X_train.shape[1]):
    print("%d. Feature %d (%f)" %(i+1, indices[i], relevants[indices[i]]))

print(std)

plt.figure(1, figsize=(9,8))
plt.title("Feature Importances")
plt.bar(range(X_train.shape[1]), relevants[indices], color="r", yerr=std[indices], align="center")
plt.xticks(range(X_train.shape[1]), X_train.columns[indices], rotation=90)
plt.xlim([-1,X_train.shape[1]])
plt.show()

# Feature Extraction through PCA
# principal component analysis

pca = PCA()
pca.fit(X_train)
PCA(copy=True, iterated_power="auto", n_components=None, random_state=None, svd_solver='auto', tol=0.0, whiten = False)
plt.figure(1, figsize=(9,8))
plt.clf()
plt.axes([.2,.2,.7,.7])
plt.plot(pca.explained_variance_ratio_, linewidth=2)
plt.axis('tight')
plt.xlabel('Number of Features')
plt.ylabel('Variance Ratio')